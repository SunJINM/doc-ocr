# 数学试卷题目分割优化方案

## 一、问题分析

### 1.1 当前状况
使用 PP-OCR-VL 进行数学试卷版面分析后，在 `result_data.json` 中得到了完整的布局检测结果，包括文本块、图像块等元素的位置和内容。

### 1.2 存在的两个核心问题

#### 问题1：题目文本与配图分离
**现象描述**：
- 题目的文本部分（如"2. 下图中有( )条线段"）和对应的图片被识别为独立的 block
- 文本 block 和图像 block 的 `block_id` 不同，`group_id` 也不同
- 需要通过空间位置关系来判断它们的归属

**具体案例**（从 result_data.json）：
```json
// 题目2的文本部分
{
    "block_label": "text",
    "block_content": "2. 下图中有( )条线段、( )条直线、( )条射线。",
    "block_bbox": [129, 1538, 2197, 1643],
    "block_id": 4,
    "group_id": 4
}

// 题目2的配图部分（分离的）
{
    "block_label": "image",
    "block_content": "",
    "block_bbox": [1341, 1778, 1808, 2109],
    "block_id": 5,
    "group_id": 5  // 注意：group_id 不同
}
```

**问题影响**：
- 无法直接确定哪张图片属于哪个题目
- 需要额外的逻辑来合并文本和图像
- 可能导致题目信息不完整或错误关联

#### 问题2：多个题目被合并到一个文本块
**现象描述**：
- 多个独立题目被识别为同一个 text block
- 题目之间缺乏明确的分隔标识

**具体案例**（从 result_data.json）：
```json
// 一个block中包含了第5题和第6题
{
    "block_label": "text",
    "block_content": "5. 右面两个正方形的边长分别是 5 厘米和 3 厘米，图中共有（）\n个梯形，其中最大的梯形的两条底分别是( )厘米和( )厘米，\n高是( )厘米。\n6. 如图是用木棍扎成的栅栏图样，图中木棍围成了( )个平行四边形，( )个梯形。",
    "block_bbox": [124, 3485, 2510, 4008],
    "block_id": 8
}
```

**问题影响**：
- 无法准确获取每个题目的独立坐标框
- 题目边界模糊，影响后续的自动批改和分析
- 难以实现题目级别的精细化处理

### 1.3 问题根源分析

**PP-OCR-VL 的工作机制**：
- **版面检测**：基于视觉特征识别文本块、图像块等元素
- **文本识别**：对检测到的文本区域进行 OCR
- **布局分析**：识别元素类型，但不理解语义关系

**导致问题的原因**：
1. **缺乏语义理解**：模型不理解"题目"的概念，只识别"文本块"和"图像块"
2. **空间分组策略**：根据空间距离进行分组，可能将相邻题目合并
3. **图文关联规则缺失**：没有专门针对试卷"题目-配图"关系的识别机制
4. **题号识别不足**：虽然能识别文本，但未将题号作为分割依据

## 二、解决方案设计

### 2.1 总体思路

```
原始检测结果
    ↓
后处理优化流程
    ↓
├─ 问题1解决：图文合并模块（使用 Qwen-VL）
│   ├─ 空间位置分析
│   ├─ 视觉模型验证
│   └─ 生成完整题目坐标框
│
└─ 问题2解决：题目拆分模块
    ├─ 题号检测与识别
    ├─ 文本块智能分割
    └─ 边界框重新计算
    ↓
优化后的结构化题目数据
```

### 2.2 问题1解决方案：基于 Qwen-VL 的图文合并

#### 2.2.1 方案概述
利用 Qwen-VL 视觉语言模型的多模态理解能力，识别题目文本与配图的语义关联，并计算合并后的统一坐标框。

#### 2.2.2 核心算法流程

```python
class QuestionImageMerger:
    """题目与图像合并处理器"""

    def __init__(self, qwen_vl_client):
        self.vl_client = qwen_vl_client  # OpenAI SDK 兼容的 Qwen-VL 客户端

    def merge_text_and_images(self, text_blocks, image_blocks, original_image):
        """
        合并文本题目和对应的图像

        Args:
            text_blocks: 文本块列表 [{'block_id', 'content', 'bbox', ...}]
            image_blocks: 图像块列表 [{'block_id', 'bbox', ...}]
            original_image: 原始试卷图像

        Returns:
            merged_questions: 合并后的题目列表，包含完整坐标框
        """
        merged_questions = []

        # 步骤1: 空间关系预分析
        for text_block in text_blocks:
            candidate_images = self._find_spatial_related_images(
                text_block, image_blocks
            )

            if not candidate_images:
                # 无配图的题目
                merged_questions.append({
                    'question_id': self._extract_question_id(text_block['content']),
                    'text_content': text_block['content'],
                    'has_image': False,
                    'bbox': text_block['bbox'],
                    'components': [text_block]
                })
                continue

            # 步骤2: 使用 Qwen-VL 进行视觉验证
            verified_images = self._verify_with_qwen_vl(
                text_block, candidate_images, original_image
            )

            # 步骤3: 计算合并后的坐标框
            merged_bbox = self._calculate_merged_bbox(
                text_block['bbox'],
                [img['bbox'] for img in verified_images]
            )

            # 步骤4: 构建完整题目结构
            merged_questions.append({
                'question_id': self._extract_question_id(text_block['content']),
                'text_content': text_block['content'],
                'has_image': True,
                'images': verified_images,
                'bbox': merged_bbox,
                'components': [text_block] + verified_images
            })

        return merged_questions

    def _find_spatial_related_images(self, text_block, image_blocks,
                                    max_vertical_distance=200):
        """
        基于空间位置查找可能相关的图像

        策略：
        1. 图像必须在文本块的下方或右侧（符合阅读习惯）
        2. 垂直距离不超过阈值
        3. 水平位置有重叠或接近
        """
        text_bbox = text_block['bbox']  # [x1, y1, x2, y2]
        text_bottom = text_bbox[3]
        text_left = text_bbox[0]
        text_right = text_bbox[2]
        text_center_x = (text_left + text_right) / 2

        candidates = []

        for img_block in image_blocks:
            img_bbox = img_block['bbox']
            img_top = img_bbox[1]
            img_left = img_bbox[0]
            img_right = img_bbox[2]
            img_center_x = (img_left + img_right) / 2

            # 条件1: 图像在文本下方
            if img_top < text_bottom:
                continue

            # 条件2: 垂直距离检查
            vertical_distance = img_top - text_bottom
            if vertical_distance > max_vertical_distance:
                continue

            # 条件3: 水平位置关联性检查
            # 情况A: 图像中心在文本水平范围内
            # 情况B: 文本中心在图像水平范围内
            # 情况C: 两者有水平重叠
            horizontal_related = (
                (text_left <= img_center_x <= text_right) or
                (img_left <= text_center_x <= img_right) or
                (img_left <= text_right and img_right >= text_left)
            )

            if horizontal_related:
                candidates.append({
                    'block': img_block,
                    'distance': vertical_distance,
                    'confidence': self._calculate_spatial_confidence(
                        text_bbox, img_bbox
                    )
                })

        # 按距离排序，优先选择最近的图像
        candidates.sort(key=lambda x: x['distance'])
        return [c['block'] for c in candidates]

    def _verify_with_qwen_vl(self, text_block, candidate_images,
                            original_image):
        """
        使用 Qwen-VL 验证图像与题目的关联性

        通过视觉语言模型理解题目内容和图像内容的语义关系
        """
        verified_images = []
        question_text = text_block['content']

        for img_block in candidate_images:
            # 裁剪图像区域
            img_bbox = img_block['bbox']
            cropped_image = self._crop_image(original_image, img_bbox)

            # 构建 Qwen-VL 提示词
            prompt = f"""
请分析以下数学题目是否需要这张图片来辅助解答：

题目内容：
{question_text}

请回答：
1. 这张图片是否与题目直接相关？（是/否）
2. 如果相关，图片在题目中的作用是什么？

请以JSON格式回答：
{{
    "is_related": true/false,
    "reason": "原因说明",
    "confidence": 0.0-1.0
}}
"""

            # 调用 Qwen-VL（OpenAI SDK 格式）
            response = self.vl_client.chat.completions.create(
                model="qwen-vl-plus",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": self._image_to_base64_url(cropped_image)
                                }
                            }
                        ]
                    }
                ],
                temperature=0.1,
                max_tokens=500
            )

            # 解析响应
            result = self._parse_vl_response(response)

            if result['is_related'] and result['confidence'] > 0.7:
                img_block['vl_verification'] = result
                verified_images.append(img_block)

        return verified_images

    def _calculate_merged_bbox(self, text_bbox, image_bboxes):
        """
        计算合并后的完整题目坐标框

        策略：取所有组件的最小外接矩形
        """
        all_bboxes = [text_bbox] + image_bboxes

        x1 = min(bbox[0] for bbox in all_bboxes)
        y1 = min(bbox[1] for bbox in all_bboxes)
        x2 = max(bbox[2] for bbox in all_bboxes)
        y2 = max(bbox[3] for bbox in all_bboxes)

        return [x1, y1, x2, y2]

    def _extract_question_id(self, text_content):
        """提取题号（如 "1.", "2.", "(1)", "第1题" 等）"""
        import re

        patterns = [
            r'^(\d+)[\.、]',           # 1. 或 1、
            r'^\((\d+)\)',             # (1)
            r'^第(\d+)题',             # 第1题
            r'^\[(\d+)\]',             # [1]
        ]

        for pattern in patterns:
            match = re.match(pattern, text_content.strip())
            if match:
                return int(match.group(1))

        return None
```

#### 2.2.3 Qwen-VL 集成示例

```python
from openai import OpenAI

# 初始化 Qwen-VL 客户端（使用 OpenAI SDK）
qwen_client = OpenAI(
    api_key="your-dashscope-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# 创建合并处理器
merger = QuestionImageMerger(qwen_client)

# 处理检测结果
parsing_results = json.load(open('output/result_data.json'))
layout_blocks = parsing_results['layoutParsingResults'][0]['prunedResult']['parsing_res_list']

# 分离文本块和图像块
text_blocks = [b for b in layout_blocks if b['block_label'] == 'text']
image_blocks = [b for b in layout_blocks if b['block_label'] == 'image']

# 加载原始图像
original_image = cv2.imread('data/shuxue/1.png')

# 执行合并
merged_questions = merger.merge_text_and_images(
    text_blocks, image_blocks, original_image
)

# 保存结果
with open('output/merged_questions.json', 'w', encoding='utf-8') as f:
    json.dump(merged_questions, f, ensure_ascii=False, indent=2)
```

#### 2.2.4 优化策略

**策略1：分阶段验证**
```python
# 先用空间规则快速筛选，再用 VL 模型精确验证
# 减少 API 调用次数，降低成本
if len(candidate_images) > 3:
    # 高置信度的空间关系直接合并
    high_confidence = [c for c in candidates if c['confidence'] > 0.9]
    if high_confidence:
        return high_confidence

    # 否则只验证最近的前3个
    candidate_images = candidate_images[:3]
```

**策略2：上下文增强**
```python
# 提供题目上下文信息，帮助模型更好理解
context_prompt = f"""
这是一份{grade}年级{subject}试卷的第{section}部分。
当前题目是第{question_id}题，类型为{question_type}。
上一题内容：{previous_question}
下一题内容：{next_question}

当前题目：{current_question}
"""
```

**策略3：批量处理**
```python
# 对多个候选图像批量调用，减少请求次数
def batch_verify_images(self, text_block, candidate_images, batch_size=4):
    """批量验证多个候选图像"""
    # 构建包含多张图片的单个请求
    # 让模型同时判断哪些图片相关
```

### 2.3 问题2解决方案：题目智能拆分

#### 2.3.1 方案概述
针对一个文本块包含多个题目的情况，通过题号识别、文本分割和边界框重计算，将其拆分为独立的题目单元。

#### 2.3.2 核心算法流程

```python
class QuestionSplitter:
    """题目拆分处理器"""

    def __init__(self, ocr_model):
        self.ocr_model = ocr_model  # PaddleOCR实例

    def split_merged_questions(self, text_block, original_image):
        """
        拆分包含多个题目的文本块

        Args:
            text_block: 包含多个题目的文本块
            original_image: 原始图像

        Returns:
            split_questions: 拆分后的独立题目列表
        """
        content = text_block['block_content']
        bbox = text_block['block_bbox']

        # 步骤1: 识别题号位置
        question_numbers = self._detect_question_numbers(content)

        if len(question_numbers) <= 1:
            # 只有一个题目或没有题号，不需要拆分
            return [text_block]

        # 步骤2: 基于OCR的精细定位
        # 裁剪文本块区域
        cropped_image = self._crop_image(original_image, bbox)

        # 对裁剪区域进行OCR，获取每个字符的精确坐标
        ocr_results = self.ocr_model.ocr(cropped_image, det=True, rec=True, cls=False)

        # 步骤3: 定位每个题号在图像中的精确位置
        question_positions = self._locate_question_numbers(
            question_numbers, ocr_results[0], bbox
        )

        # 步骤4: 根据题号位置分割文本块
        split_questions = self._split_by_positions(
            text_block, question_positions, question_numbers
        )

        return split_questions

    def _detect_question_numbers(self, text):
        """
        检测文本中的所有题号

        返回：[(题号, 在文本中的起始位置, 匹配的字符串)]
        """
        import re

        question_numbers = []

        # 正则模式：匹配常见题号格式
        patterns = [
            (r'(\d+)[\.、]\s*', 'arabic_dot'),      # 1. 2、
            (r'\((\d+)\)\s*', 'arabic_paren'),      # (1) (2)
            (r'第(\d+)题\s*', 'chinese_prefix'),    # 第1题
            (r'\[(\d+)\]\s*', 'arabic_bracket'),    # [1]
        ]

        for pattern, pattern_type in patterns:
            for match in re.finditer(pattern, text):
                number = int(match.group(1))
                position = match.start()
                matched_str = match.group(0)

                question_numbers.append({
                    'number': number,
                    'position': position,
                    'matched_str': matched_str,
                    'type': pattern_type
                })

        # 按位置排序
        question_numbers.sort(key=lambda x: x['position'])

        # 过滤重复（同一位置可能被多个模式匹配）
        filtered = []
        last_pos = -10
        for qn in question_numbers:
            if qn['position'] - last_pos > 5:  # 位置差距大于5个字符
                filtered.append(qn)
                last_pos = qn['position']

        return filtered

    def _locate_question_numbers(self, question_numbers, ocr_result, base_bbox):
        """
        定位题号在原图中的精确坐标

        Args:
            question_numbers: 检测到的题号信息
            ocr_result: OCR结果（包含每行文本和坐标）
            base_bbox: 文本块在原图中的坐标

        Returns:
            question_positions: [{number, bbox_in_original}]
        """
        positions = []

        for qn_info in question_numbers:
            target_text = qn_info['matched_str'].strip()

            # 在OCR结果中查找匹配的文本
            for line_result in ocr_result:
                line_bbox = line_result[0]  # [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                line_text = line_result[1][0]

                if target_text in line_text or \
                   line_text.startswith(str(qn_info['number'])):
                    # 找到匹配的行
                    # 将相对坐标转换为原图坐标
                    original_bbox = self._convert_to_original_bbox(
                        line_bbox, base_bbox
                    )

                    positions.append({
                        'number': qn_info['number'],
                        'bbox': original_bbox,
                        'text': line_text
                    })
                    break

        return positions

    def _split_by_positions(self, text_block, question_positions, question_numbers):
        """
        根据题号位置分割文本块

        策略：
        1. 按题号的Y坐标（垂直位置）划分边界
        2. 每个题目的bbox从该题号开始到下一题号之前
        """
        split_questions = []
        content = text_block['block_content']
        base_bbox = text_block['block_bbox']

        # 按Y坐标排序题号位置
        sorted_positions = sorted(question_positions,
                                 key=lambda x: x['bbox'][1])  # 按top坐标排序

        for i, qn_pos in enumerate(sorted_positions):
            # 确定当前题目的内容范围
            current_qn = question_numbers[i]

            # 内容提取：从当前题号到下一题号之前
            if i < len(sorted_positions) - 1:
                next_qn = question_numbers[i + 1]
                # 从文本中提取
                question_content = content[
                    current_qn['position']:next_qn['position']
                ].strip()
            else:
                # 最后一个题目，到文本块结尾
                question_content = content[
                    current_qn['position']:
                ].strip()

            # 计算题目的bbox
            top = qn_pos['bbox'][1]

            if i < len(sorted_positions) - 1:
                # 下一题号的top作为当前题目的bottom
                bottom = sorted_positions[i + 1]['bbox'][1]
            else:
                # 最后一题，使用原文本块的bottom
                bottom = base_bbox[3]

            question_bbox = [
                base_bbox[0],  # left: 使用原文本块的left
                top,           # top: 当前题号的top
                base_bbox[2],  # right: 使用原文本块的right
                bottom         # bottom: 下一题号的top或原块的bottom
            ]

            # 构建独立题目
            split_questions.append({
                'block_label': 'text',
                'block_content': question_content,
                'block_bbox': question_bbox,
                'question_number': current_qn['number'],
                'original_block_id': text_block['block_id'],
                'split_from_merged': True
            })

        return split_questions

    def _convert_to_original_bbox(self, relative_bbox, base_bbox):
        """
        将OCR的相对坐标转换为原图坐标

        Args:
            relative_bbox: OCR返回的坐标（相对于裁剪图像）
            base_bbox: 裁剪区域在原图中的坐标 [x1, y1, x2, y2]

        Returns:
            original_bbox: 原图坐标 [x1, y1, x2, y2]
        """
        # OCR bbox格式: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
        # 取最小外接矩形
        xs = [point[0] for point in relative_bbox]
        ys = [point[1] for point in relative_bbox]

        rel_x1, rel_y1 = min(xs), min(ys)
        rel_x2, rel_y2 = max(xs), max(ys)

        # 转换到原图坐标系
        orig_x1 = base_bbox[0] + rel_x1
        orig_y1 = base_bbox[1] + rel_y1
        orig_x2 = base_bbox[0] + rel_x2
        orig_y2 = base_bbox[1] + rel_y2

        return [orig_x1, orig_y1, orig_x2, orig_y2]
```

#### 2.3.3 边界情况处理

**情况1：题号不规范**
```python
# 处理题号缺失或格式不一致的情况
def _infer_missing_question_numbers(self, detected_numbers, text):
    """
    推断缺失的题号

    策略：
    1. 检查题号序列的连续性（如 1, 2, 4 → 缺少3）
    2. 根据上下文推断题号格式
    3. 使用换行符、段落结构辅助判断
    """
    if not detected_numbers:
        return detected_numbers

    # 检查序列连续性
    numbers = sorted([qn['number'] for qn in detected_numbers])

    if len(numbers) < 2:
        return detected_numbers

    # 检测缺失的题号
    expected_range = range(numbers[0], numbers[-1] + 1)
    missing = [n for n in expected_range if n not in numbers]

    if missing:
        # 尝试通过段落分割推断缺失题号的位置
        paragraphs = text.split('\n\n')  # 双换行作为段落分隔
        # ... 进一步推断逻辑

    return detected_numbers
```

**情况2：题目内容包含数字干扰**
```python
# 避免将题目内容中的数字误识别为题号
def _validate_question_number(self, candidate, context):
    """
    验证候选题号的有效性

    判断标准：
    1. 位置：应该在行首或段落开始
    2. 后续：后面应该跟随题目描述性文本
    3. 连续性：与前后题号形成合理序列
    """
    # 检查是否在行首
    if not self._is_at_line_start(candidate, context):
        return False

    # 检查后续文本特征
    following_text = context[candidate['position'] + len(candidate['matched_str']):]
    if not self._is_question_content(following_text):
        return False

    return True
```

**情况3：复杂题目结构（小题）**
```python
# 处理带有小题的复杂题目（如 1. (1) (2) (3)）
def _handle_sub_questions(self, main_question, text):
    """
    识别和处理小题

    策略：
    1. 检测小题标识（(1), ①, a. 等）
    2. 判断是否属于主题还是独立题目
    3. 构建层级结构
    """
    sub_question_patterns = [
        r'\(([1-9])\)',      # (1) (2)
        r'([①②③④⑤⑥⑦⑧⑨])',  # 带圈数字
        r'([a-d])\.',        # a. b. c. d.
    ]

    # ... 小题识别和结构化逻辑
```

#### 2.3.4 完整拆分示例

```python
from paddleocr import PaddleOCR

# 初始化OCR
ocr = PaddleOCR(use_angle_cls=True, lang='ch', det=True, rec=True)

# 创建拆分器
splitter = QuestionSplitter(ocr)

# 加载原始图像
original_image = cv2.imread('data/shuxue/1.png')

# 处理合并的题目块
merged_block = {
    'block_label': 'text',
    'block_content': '''5. 右面两个正方形的边长分别是 5 厘米和 3 厘米，图中共有（）
个梯形，其中最大的梯形的两条底分别是( )厘米和( )厘米，
高是( )厘米。
6. 如图是用木棍扎成的栅栏图样，图中木棍围成了( )个平行四边形，( )个梯形。''',
    'block_bbox': [124, 3485, 2510, 4008],
    'block_id': 8
}

# 执行拆分
split_questions = splitter.split_merged_questions(merged_block, original_image)

# 输出结果
for question in split_questions:
    print(f"题号: {question['question_number']}")
    print(f"内容: {question['block_content']}")
    print(f"坐标: {question['block_bbox']}")
    print("---")
```

### 2.4 完整流程整合

```python
class ExamPaperQuestionExtractor:
    """试卷题目提取完整流程"""

    def __init__(self, qwen_vl_client, ocr_model):
        self.merger = QuestionImageMerger(qwen_vl_client)
        self.splitter = QuestionSplitter(ocr_model)

    def extract_questions(self, result_data_path, original_image_path):
        """
        完整的题目提取流程

        流程：
        1. 加载PP-OCR-VL检测结果
        2. 拆分合并的题目块
        3. 合并题目文本与配图
        4. 生成最终的结构化题目数据
        """
        # 步骤1: 加载数据
        with open(result_data_path, 'r', encoding='utf-8') as f:
            result_data = json.load(f)

        original_image = cv2.imread(original_image_path)

        parsing_results = result_data['layoutParsingResults'][0]['prunedResult']
        layout_blocks = parsing_results['parsing_res_list']

        # 步骤2: 分类处理
        text_blocks = [b for b in layout_blocks if b['block_label'] == 'text']
        image_blocks = [b for b in layout_blocks if b['block_label'] == 'image']
        title_blocks = [b for b in layout_blocks
                       if b['block_label'] in ['doc_title', 'paragraph_title']]

        # 步骤3: 拆分合并的题目
        all_text_blocks = []
        for text_block in text_blocks:
            split_results = self.splitter.split_merged_questions(
                text_block, original_image
            )
            all_text_blocks.extend(split_results)

        # 步骤4: 合并题目与图像
        merged_questions = self.merger.merge_text_and_images(
            all_text_blocks, image_blocks, original_image
        )

        # 步骤5: 构建最终结构
        structured_exam = {
            'exam_info': self._extract_exam_info(title_blocks),
            'questions': self._organize_questions(merged_questions),
            'metadata': {
                'total_questions': len(merged_questions),
                'with_images': len([q for q in merged_questions if q['has_image']]),
                'processing_timestamp': datetime.now().isoformat()
            }
        }

        return structured_exam

    def _organize_questions(self, merged_questions):
        """
        组织题目结构

        功能：
        1. 按题号排序
        2. 识别题目类型（选择、填空、解答等）
        3. 提取分数信息
        4. 构建层级结构（大题-小题）
        """
        organized = []

        # 按题号排序
        sorted_questions = sorted(
            merged_questions,
            key=lambda q: q.get('question_number', 999)
        )

        for question in sorted_questions:
            content = question['text_content']

            # 识别题目类型
            question_type = self._identify_question_type(content)

            # 提取分数
            score = self._extract_score(content)

            organized.append({
                'id': question['question_number'],
                'type': question_type,
                'score': score,
                'content': {
                    'text': content,
                    'has_image': question['has_image'],
                    'images': question.get('images', [])
                },
                'bbox': question['bbox']
            })

        return organized

    def _identify_question_type(self, content):
        """识别题目类型"""
        if re.search(r'[A-D]\.', content):
            return '选择题'
        elif re.search(r'[（(][\s]*[）)]', content):
            return '填空题'
        elif re.search(r'判断|对错|√|×', content):
            return '判断题'
        elif re.search(r'计算|求|解', content):
            return '计算题'
        else:
            return '解答题'

    def _extract_score(self, content):
        """提取分数信息"""
        patterns = [
            r'(\d+)分',
            r'每题(\d+)分',
            r'共(\d+)分'
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                return int(match.group(1))

        return 0
```

## 三、技术实现细节

### 3.1 Qwen-VL API 配置

```python
# config.py
QWEN_VL_CONFIG = {
    'api_key': 'your-dashscope-api-key',
    'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',
    'model': 'qwen-vl-plus',  # 或 'qwen-vl-max'
    'temperature': 0.1,       # 低温度提高稳定性
    'max_tokens': 500,
    'timeout': 30
}

# 成本优化建议
# 1. 优先使用空间规则筛选，减少VL调用
# 2. 批量处理多个候选图像
# 3. 对高置信度的空间关联跳过VL验证
# 4. 缓存VL结果，避免重复调用
```

### 3.2 PaddleOCR 精细配置

```python
# OCR配置用于题号定位
OCR_CONFIG = {
    'use_angle_cls': True,    # 检测文字角度
    'lang': 'ch',             # 中文
    'det': True,              # 启用检测
    'rec': True,              # 启用识别
    'cls': False,             # 不需要方向分类（已有angle_cls）
    'use_gpu': True,          # 使用GPU加速
    'det_db_thresh': 0.3,     # 检测阈值（降低以检测小字）
    'det_db_box_thresh': 0.5, # 框选阈值
    'rec_batch_num': 6        # 识别批次大小
}
```

### 3.3 数据结构设计

```python
# 优化后的题目数据结构
QuestionSchema = {
    'id': int,                          # 题号
    'type': str,                        # 题目类型
    'score': int,                       # 分值
    'content': {
        'text': str,                    # 文本内容
        'has_image': bool,              # 是否有配图
        'images': [                     # 配图列表
            {
                'block_id': int,
                'bbox': [int, int, int, int],
                'image_path': str,      # 图片路径
                'description': str,     # 图片描述（可选）
                'vl_verified': bool,    # 是否经过VL验证
                'confidence': float     # 关联置信度
            }
        ]
    },
    'bbox': [int, int, int, int],      # 完整题目坐标框
    'sub_questions': [],                # 小题（如有）
    'metadata': {
        'split_from_merged': bool,      # 是否来自拆分
        'original_block_ids': [int],    # 原始block ID列表
        'processing_notes': str         # 处理备注
    }
}
```

### 3.4 性能优化策略

#### 3.4.1 并行处理
```python
from concurrent.futures import ThreadPoolExecutor
import asyncio

class ParallelQuestionProcessor:
    """并行处理题目提取"""

    def __init__(self, max_workers=4):
        self.max_workers = max_workers

    def process_questions_parallel(self, text_blocks, image_blocks, original_image):
        """并行处理多个题目块"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # 并行拆分题目
            split_futures = [
                executor.submit(self.splitter.split_merged_questions, block, original_image)
                for block in text_blocks
            ]

            # 收集拆分结果
            all_text_blocks = []
            for future in split_futures:
                all_text_blocks.extend(future.result())

            # 并行合并图文（每个题目独立处理）
            merge_futures = [
                executor.submit(
                    self.merger.merge_text_and_images,
                    [text_block],
                    image_blocks,
                    original_image
                )
                for text_block in all_text_blocks
            ]

            merged_questions = []
            for future in merge_futures:
                merged_questions.extend(future.result())

        return merged_questions
```

#### 3.4.2 缓存机制
```python
import hashlib
import pickle
from functools import lru_cache

class CachedVLVerifier:
    """带缓存的VL验证器"""

    def __init__(self, cache_dir='cache/vl_results'):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def verify_with_cache(self, text_content, image_data):
        """使用缓存的VL验证"""
        # 生成缓存键
        cache_key = self._generate_cache_key(text_content, image_data)
        cache_path = os.path.join(self.cache_dir, f"{cache_key}.pkl")

        # 检查缓存
        if os.path.exists(cache_path):
            with open(cache_path, 'rb') as f:
                return pickle.load(f)

        # 调用VL模型
        result = self._call_vl_model(text_content, image_data)

        # 保存缓存
        with open(cache_path, 'wb') as f:
            pickle.dump(result, f)

        return result

    def _generate_cache_key(self, text, image_data):
        """生成缓存键"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        image_hash = hashlib.md5(image_data).hexdigest()
        return f"{text_hash}_{image_hash}"
```

#### 3.4.3 分阶段处理
```python
class StageBasedExtractor:
    """分阶段题目提取器"""

    def extract_with_stages(self, result_data, original_image):
        """
        分阶段处理，快速失败

        阶段1: 快速空间分析（无API调用）
        阶段2: 规则拆分（仅OCR）
        阶段3: VL验证（仅对不确定的情况）
        """
        # 阶段1: 空间分析
        spatial_groups = self._spatial_grouping(result_data)

        high_confidence = []
        low_confidence = []

        for group in spatial_groups:
            if group['confidence'] > 0.9:
                high_confidence.append(group)
            else:
                low_confidence.append(group)

        # 高置信度直接通过
        final_questions = self._finalize_groups(high_confidence)

        # 低置信度进入VL验证
        if low_confidence:
            verified_questions = self._vl_verify_groups(low_confidence)
            final_questions.extend(verified_questions)

        return final_questions
```

## 四、测试与验证方案

### 4.1 测试数据集设计

```python
# 测试用例分类
test_cases = {
    '基础场景': [
        {
            'name': '简单题目无配图',
            'file': 'test_simple_no_image.png',
            'expected': {
                'question_count': 5,
                'with_image': 0,
                'merged_blocks': 0
            }
        },
        {
            'name': '题目带单张配图',
            'file': 'test_single_image.png',
            'expected': {
                'question_count': 3,
                'with_image': 3,
                'merged_blocks': 0
            }
        }
    ],
    '复杂场景': [
        {
            'name': '多题目合并块',
            'file': 'test_merged_questions.png',
            'expected': {
                'question_count': 6,
                'split_count': 2,  # 2个块被拆分
                'merged_blocks': 1
            }
        },
        {
            'name': '题目多张配图',
            'file': 'test_multiple_images.png',
            'expected': {
                'question_count': 2,
                'images_per_question': 4  # 选择题ABCD四个选项图
            }
        }
    ],
    '边界场景': [
        {
            'name': '题号不规范',
            'file': 'test_irregular_numbers.png',
            'expected': {
                'question_count': 4,
                'inferred_numbers': 1  # 1个题号需要推断
            }
        },
        {
            'name': '图文距离远',
            'file': 'test_distant_image.png',
            'expected': {
                'question_count': 2,
                'vl_verified': 2  # 需要VL验证
            }
        }
    ]
}
```

### 4.2 评估指标

```python
class ExtractionEvaluator:
    """题目提取评估器"""

    def evaluate(self, extracted_questions, ground_truth):
        """
        评估提取结果

        指标：
        1. 题目检测准确率：正确识别的题目数 / 总题目数
        2. 边界框IoU：预测框与真实框的交并比
        3. 图文关联准确率：正确关联的图像数 / 总图像数
        4. 拆分准确率：正确拆分的题目数 / 应拆分的题目数
        """
        metrics = {
            'detection_precision': 0,    # 检测精确率
            'detection_recall': 0,       # 检测召回率
            'bbox_iou_mean': 0,          # 平均IoU
            'image_association_acc': 0,  # 图像关联准确率
            'split_accuracy': 0,         # 拆分准确率
            'end_to_end_acc': 0          # 端到端准确率
        }

        # 1. 题目检测评估
        detected_ids = set(q['id'] for q in extracted_questions)
        ground_truth_ids = set(q['id'] for q in ground_truth)

        tp = len(detected_ids & ground_truth_ids)  # 真阳性
        fp = len(detected_ids - ground_truth_ids)  # 假阳性
        fn = len(ground_truth_ids - detected_ids)  # 假阴性

        metrics['detection_precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0
        metrics['detection_recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0

        # 2. 边界框评估
        ious = []
        for extracted in extracted_questions:
            for gt in ground_truth:
                if extracted['id'] == gt['id']:
                    iou = self._calculate_iou(extracted['bbox'], gt['bbox'])
                    ious.append(iou)
                    break

        metrics['bbox_iou_mean'] = sum(ious) / len(ious) if ious else 0

        # 3. 图像关联评估
        image_correct = 0
        image_total = 0

        for extracted in extracted_questions:
            for gt in ground_truth:
                if extracted['id'] == gt['id']:
                    extracted_images = set(
                        img['block_id'] for img in extracted['content'].get('images', [])
                    )
                    gt_images = set(
                        img['block_id'] for img in gt['content'].get('images', [])
                    )

                    if extracted_images == gt_images:
                        image_correct += 1
                    image_total += 1
                    break

        metrics['image_association_acc'] = (
            image_correct / image_total if image_total > 0 else 0
        )

        return metrics

    def _calculate_iou(self, bbox1, bbox2):
        """计算两个边界框的IoU"""
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])

        if x2 < x1 or y2 < y1:
            return 0.0

        intersection = (x2 - x1) * (y2 - y1)
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0.0
```

### 4.3 可视化验证

```python
import cv2
import matplotlib.pyplot as plt

class ResultVisualizer:
    """结果可视化工具"""

    def visualize_extraction_results(self, original_image, extracted_questions,
                                    output_path='output/visualization.jpg'):
        """
        可视化题目提取结果

        功能：
        1. 在原图上绘制题目边界框
        2. 标注题号
        3. 用不同颜色区分有无配图
        4. 显示图文关联线
        """
        vis_image = original_image.copy()

        for question in extracted_questions:
            bbox = question['bbox']
            question_id = question['id']
            has_image = question['content']['has_image']

            # 选择颜色
            color = (0, 255, 0) if has_image else (255, 0, 0)  # 绿色=有图，红色=无图

            # 绘制题目边界框
            cv2.rectangle(
                vis_image,
                (int(bbox[0]), int(bbox[1])),
                (int(bbox[2]), int(bbox[3])),
                color,
                3
            )

            # 标注题号
            cv2.putText(
                vis_image,
                f"Q{question_id}",
                (int(bbox[0]), int(bbox[1]) - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                1.5,
                color,
                3
            )

            # 绘制配图及关联线
            if has_image:
                for img_info in question['content']['images']:
                    img_bbox = img_info['bbox']

                    # 绘制图像框
                    cv2.rectangle(
                        vis_image,
                        (int(img_bbox[0]), int(img_bbox[1])),
                        (int(img_bbox[2]), int(img_bbox[3])),
                        (0, 255, 255),  # 黄色
                        2
                    )

                    # 绘制关联线
                    text_center = (
                        int((bbox[0] + bbox[2]) / 2),
                        int((bbox[1] + bbox[3]) / 2)
                    )
                    img_center = (
                        int((img_bbox[0] + img_bbox[2]) / 2),
                        int((img_bbox[1] + img_bbox[3]) / 2)
                    )

                    cv2.line(
                        vis_image,
                        text_center,
                        img_center,
                        (255, 255, 0),  # 青色
                        2
                    )

        # 保存可视化结果
        cv2.imwrite(output_path, vis_image)

        # 显示统计信息
        total = len(extracted_questions)
        with_images = len([q for q in extracted_questions if q['content']['has_image']])

        print(f"=== 提取结果统计 ===")
        print(f"总题目数: {total}")
        print(f"带配图: {with_images}")
        print(f"无配图: {total - with_images}")
        print(f"可视化图片已保存: {output_path}")

        return vis_image
```

## 五、部署与集成

### 5.1 完整流程脚本

```python
# main_extraction.py
"""
数学试卷题目提取主流程
"""
import os
import json
import cv2
from openai import OpenAI
from paddleocr import PaddleOCR

from question_merger import QuestionImageMerger
from question_splitter import QuestionSplitter
from extractor import ExamPaperQuestionExtractor
from visualizer import ResultVisualizer

def main():
    # 1. 配置初始化
    print("正在初始化模型...")

    # Qwen-VL客户端
    qwen_client = OpenAI(
        api_key=os.environ.get('DASHSCOPE_API_KEY'),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    # PaddleOCR
    ocr = PaddleOCR(use_angle_cls=True, lang='ch', use_gpu=True)

    # 2. 加载数据
    print("正在加载检测结果...")
    result_data_path = 'output/result_data.json'
    original_image_path = 'data/shuxue/1.png'

    with open(result_data_path, 'r', encoding='utf-8') as f:
        result_data = json.load(f)

    original_image = cv2.imread(original_image_path)

    # 3. 创建提取器
    extractor = ExamPaperQuestionExtractor(qwen_client, ocr)

    # 4. 执行提取
    print("正在提取题目...")
    structured_exam = extractor.extract_questions(
        result_data_path,
        original_image_path
    )

    # 5. 保存结果
    output_path = 'output/structured_questions.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(structured_exam, f, ensure_ascii=False, indent=2)

    print(f"提取完成！共提取 {len(structured_exam['questions'])} 道题目")
    print(f"结果已保存至: {output_path}")

    # 6. 可视化
    print("正在生成可视化结果...")
    visualizer = ResultVisualizer()
    visualizer.visualize_extraction_results(
        original_image,
        structured_exam['questions'],
        'output/extraction_visualization.jpg'
    )

    # 7. 输出统计信息
    print("\n=== 详细统计 ===")
    print(f"试卷标题: {structured_exam['exam_info'].get('title', 'N/A')}")
    print(f"总题目数: {structured_exam['metadata']['total_questions']}")
    print(f"带配图题目: {structured_exam['metadata']['with_images']}")

    question_types = {}
    for q in structured_exam['questions']:
        qtype = q['type']
        question_types[qtype] = question_types.get(qtype, 0) + 1

    print("\n题目类型分布:")
    for qtype, count in question_types.items():
        print(f"  {qtype}: {count}")

if __name__ == '__main__':
    main()
```

### 5.2 配置管理

```python
# config.py
"""
配置管理模块
"""
import os
from dataclasses import dataclass

@dataclass
class QwenVLConfig:
    """Qwen-VL配置"""
    api_key: str = os.environ.get('DASHSCOPE_API_KEY', '')
    base_url: str = 'https://dashscope.aliyuncs.com/compatible-mode/v1'
    model: str = 'qwen-vl-plus'
    temperature: float = 0.1
    max_tokens: int = 500
    timeout: int = 30

@dataclass
class OCRConfig:
    """OCR配置"""
    use_angle_cls: bool = True
    lang: str = 'ch'
    use_gpu: bool = True
    det: bool = True
    rec: bool = True
    det_db_thresh: float = 0.3
    det_db_box_thresh: float = 0.5

@dataclass
class ProcessingConfig:
    """处理配置"""
    max_vertical_distance: int = 200  # 图文最大垂直距离
    spatial_confidence_threshold: float = 0.9  # 空间置信度阈值
    vl_confidence_threshold: float = 0.7  # VL验证置信度阈值
    enable_cache: bool = True  # 启用缓存
    cache_dir: str = 'cache'
    parallel_workers: int = 4  # 并行worker数量

# 全局配置实例
qwen_vl_config = QwenVLConfig()
ocr_config = OCRConfig()
processing_config = ProcessingConfig()
```

### 5.3 错误处理

```python
# error_handler.py
"""
错误处理模块
"""
import logging
from functools import wraps

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ExtractionError(Exception):
    """题目提取基础异常"""
    pass

class VLVerificationError(ExtractionError):
    """VL验证异常"""
    pass

class OCRError(ExtractionError):
    """OCR处理异常"""
    pass

class QuestionSplitError(ExtractionError):
    """题目拆分异常"""
    pass

def retry_on_failure(max_retries=3, delay=1):
    """失败重试装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"{func.__name__} 失败（已重试{max_retries}次）: {e}")
                        raise
                    else:
                        logger.warning(f"{func.__name__} 失败，正在重试 ({attempt + 1}/{max_retries}): {e}")
                        time.sleep(delay)
        return wrapper
    return decorator

def safe_extraction(func):
    """安全提取装饰器"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except VLVerificationError as e:
            logger.error(f"VL验证错误: {e}")
            # 降级策略：仅使用空间规则
            return None
        except OCRError as e:
            logger.error(f"OCR错误: {e}")
            # 降级策略：保持原始块不拆分
            return args[0] if args else None
        except Exception as e:
            logger.error(f"未知错误: {e}")
            raise ExtractionError(f"提取过程中发生错误: {e}")

    return wrapper
```

## 六、预期效果与指标

### 6.1 性能指标

| 指标 | 目标值 | 备注 |
|-----|-------|------|
| 题目检测准确率 | ≥95% | 正确识别的题目数 / 总题目数 |
| 边界框IoU | ≥0.85 | 平均交并比 |
| 图文关联准确率 | ≥90% | 正确关联的图像数 / 总图像数 |
| 拆分准确率 | ≥92% | 正确拆分的题目数 / 应拆分题目数 |
| 端到端准确率 | ≥88% | 所有环节都正确的题目占比 |
| 处理速度 | ≤45秒/页 | 包含VL调用的完整流程 |

### 6.2 成本估算

**Qwen-VL API成本**（基于通义千问定价）：
- 模型：qwen-vl-plus
- 定价：约 0.008元/千tokens（图文混合）
- 单次调用：约 500 tokens
- 单次成本：约 0.004元

**典型试卷成本**：
- 假设：10道题目，其中5道有配图
- 空间筛选后需VL验证：3次
- 总成本：3 × 0.004 = 0.012元/份试卷

**优化后成本**：
- 使用缓存和分阶段处理
- 预计减少50%的VL调用
- 优化后成本：约 0.006元/份试卷

### 6.3 改进效果对比

| 方面 | 优化前 | 优化后 | 提升 |
|-----|-------|-------|-----|
| 问题1：图文分离 | 无法关联 | 90%准确关联 | 显著改善 |
| 问题2：题目合并 | 1个框包含多题 | 92%准确拆分 | 大幅提升 |
| 题目边界准确性 | IoU ≈ 0.65 | IoU ≈ 0.85 | +30% |
| 后续处理可行性 | 困难 | 可直接使用 | 质的飞跃 |

## 七、风险与应对

### 7.1 技术风险

| 风险 | 影响 | 概率 | 应对措施 |
|-----|------|------|---------|
| Qwen-VL API不稳定 | VL验证失败 | 低 | 实现降级策略，仅用空间规则 |
| OCR定位不准确 | 题号位置偏差 | 中 | 多模式匹配，增加容错范围 |
| 题号格式多变 | 识别失败 | 中 | 扩充正则模式库，人工规则补充 |
| 图像质量差 | 整体准确率下降 | 中 | 增强预处理，设置质量检测 |

### 7.2 成本风险

| 风险 | 应对 |
|-----|------|
| VL调用次数过多 | 三级优化：空间规则 → 缓存 → 批量调用 |
| API费用超预算 | 设置调用上限，超限告警 |
| 处理时间过长 | 并行处理，异步调用 |

### 7.3 数据风险

| 风险 | 应对 |
|-----|------|
| 试卷格式不规范 | 建立格式库，模式自适应 |
| 手写试卷 | OCR模型切换，增强预处理 |
| 复杂排版 | 人工审核机制，置信度阈值 |

## 八、实施计划

### 8.1 开发阶段（3周）

**第1周：核心功能开发**
- Day 1-2: QuestionImageMerger 空间分析模块
- Day 3-4: Qwen-VL集成与验证逻辑
- Day 5-7: QuestionSplitter 题号检测与拆分

**第2周：优化与集成**
- Day 1-2: 并行处理与缓存机制
- Day 3-4: 完整流程整合
- Day 5-7: 错误处理与降级策略

**第3周：测试与完善**
- Day 1-3: 单元测试与集成测试
- Day 4-5: 性能优化
- Day 6-7: 文档完善与代码审查

### 8.2 测试阶段（1周）

- 准备测试数据集（50份不同类型试卷）
- 执行功能测试
- 性能测试
- 边界场景测试
- Bug修复

### 8.3 部署阶段（3天）

- 环境配置
- 依赖安装
- 配置文件调整
- 部署验证
- 监控设置

## 九、后续优化方向

### 9.1 短期优化（1个月内）

1. **模型微调**
   - 收集真实试卷数据
   - 微调Qwen-VL提升准确率
   - 减少对通用模型的依赖

2. **规则库扩充**
   - 积累更多题号格式模式
   - 建立学科特定规则
   - 优化空间关联算法

3. **性能提升**
   - 实现更高效的并行策略
   - 优化缓存命中率
   - 减少不必要的VL调用

### 9.2 中期扩展（3个月内）

1. **多学科支持**
   - 物理：公式、实验图识别
   - 化学：方程式、结构式识别
   - 英语：听力题标记、阅读理解段落

2. **复杂结构支持**
   - 大题-小题层级结构
   - 综合题的分段识别
   - 附加题、选做题标识

3. **交互式修正**
   - Web界面可视化
   - 人工修正接口
   - 修正结果反馈训练

### 9.3 长期演进（6个月以上）

1. **端到端模型**
   - 训练专用的试卷版面分析模型
   - 直接输出题目级结构
   - 减少后处理依赖

2. **智能理解**
   - 题目难度评估
   - 知识点自动标注
   - 题目相似度计算

3. **多模态增强**
   - 视频题目支持
   - 音频题目识别
   - 3D图形理解

## 十、总结

本方案针对数学试卷题目分割中的两个核心问题，提出了系统化的解决方案：

**问题1（图文分离）解决方案**：
- 通过空间位置分析快速筛选候选图像
- 使用Qwen-VL视觉模型进行语义验证
- 计算合并后的完整题目坐标框
- 预期准确率：≥90%

**问题2（题目合并）解决方案**：
- 基于正则表达式的题号检测
- 利用OCR精细定位题号坐标
- 智能拆分文本块并重新计算边界
- 预期准确率：≥92%

**技术特点**：
1. **分阶段处理**：空间规则 → OCR定位 → VL验证，降低成本
2. **智能降级**：关键步骤失败时自动降级到备选方案
3. **性能优化**：并行处理、缓存机制、批量调用
4. **可扩展性**：模块化设计，易于扩展到其他学科

**预期成效**：
- 端到端准确率：≥88%
- 处理速度：≤45秒/页
- 单份试卷成本：≈0.006元
- 大幅提升后续自动批改和分析的可行性

该方案已充分考虑实际应用中的各种场景和边界情况，具有良好的鲁棒性和实用性。
